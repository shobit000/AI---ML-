{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "Yfr_Vlr8HBkt",
        "hwyV_J3ipUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "\n",
        "##### **Team Member  -2210992272**\n",
        "##### **Team Member  -2210992276**\n",
        "##### **Team Member  -2210992292**\n",
        "##### **Team Member  -2210992335**\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credit Card Fraud Detection**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Credit Card Fraud Detection project aims to develop a robust system for identifying and preventing fraudulent transactions in real-time. In an era where digital transactions are prevalent, ensuring the security of credit card transactions is crucial for financial institutions and cardholders alike.\n",
        "\n",
        "The project utilizes advanced machine learning algorithms to analyze transaction data and detect patterns associated with fraudulent activities. The dataset comprises a mix of legitimate and fraudulent transactions, providing a diverse training ground for the model. Feature engineering plays a significant role in extracting relevant information, including transaction amount, location, time, and user behavior.\n",
        "\n",
        "One of the key components of the project is the implementation of a supervised learning model, such as a Random Forest or Support Vector Machine, trained on historical data. The model learns to differentiate between normal and fraudulent transactions based on various features. Continuous refinement and optimization of the model are necessary to adapt to evolving fraud patterns.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop an efficient credit card fraud detection system using machine learning to identify and prevent unauthorized transactions. The project aims to enhance security, protect financial assets, and ensure the trustworthiness of digital transactions for both financial institutions andÂ cardholders."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('creditcard.csv');"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info();"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df=pd.DataFrame(df)\n",
        "print(df.duplicated())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6h0lgjw962C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.iloc[:,[0,25]]"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "\n",
        "# Print or display the unique values\n",
        "print(\"Unique values for each variable:\")\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('creditcard.csv')  # Replace with the actual file path\n",
        "\n",
        "# Explore the dataset\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our hypothetical scenario comparing exam scores between students taught using the new method and those taught using the old method, here are the manipulations and insights that could be explored:\n",
        "\n",
        "Data Collection: Gather exam scores for students in both groups, ensuring that the data is accurate and reliable.\n",
        "\n",
        "Data Cleaning: Remove any duplicate or inconsistent data, handle missing values appropriately, and ensure that the data is ready for analysis.\n",
        "\n",
        "Data Splitting: Split the data into training and testing sets if machine learning models are used for analysis, ensuring that the split is appropriate for the dataset size.\n",
        "\n",
        "Statistical Analysis: Conduct statistical tests such as t-tests or ANOVA to compare the exam scores between the two groups and determine if there is a significant difference.\n",
        "\n",
        "Visualization: Create visualizations such as box plots or histograms to compare the distribution of exam scores between the two groups and identify any patterns or trends.\n",
        "\n",
        "Feature Engineering: Create new features from the data that may be relevant for analysis, such as calculating the average exam score for each student across multiple exams.\n",
        "\n",
        "Model Building: Build machine learning models, if applicable, to predict exam scores based on the teaching method used and other relevant features.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The new teaching method may lead to higher average exam scores compared to the old method, as indicated by the statistical analysis.\n",
        "The distribution of exam scores for students taught using the new method may be more consistent, with fewer outliers compared to the old method.\n",
        "Certain student characteristics or study habits may have a significant impact on exam scores, which can be identified through feature engineering and statistical analysis.\n",
        "Overall, these manipulations and insights can help provide a deeper understanding of the effectiveness of the new teaching method and guide future educational strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Single  line chart time vs amont\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['Time'],color='blue',marker='o', linestyle='-',markersize=1)\n",
        "# plt.plot(df['Amount'], color='red', marker='o', linestyle=':',markersize=5)\n",
        "plt.title('Credit Card Transactions Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Amount')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. *Clear Proportional Representation:*\n",
        "   - A pie chart makes it easy to see the proportional distribution of fraudulent and non-fraudulent classes. Each slice represents a class, and the size of the slice corresponds to its percentage of the whole.\n",
        "\n",
        "2. *Highlighting Imbalance:*\n",
        "   - If there's a significant class imbalance, a pie chart visually emphasizes the disparities between the classes, providing a quick insight into the imbalance issue.\n",
        "\n",
        "3. *Intuitive Understanding:*\n",
        "   - Pie charts are familiar and intuitive, making it accessible for a broad audience to quickly grasp the distribution patterns.\n",
        "\n",
        "4. *Percentage Labels:*\n",
        "   - Including percentage labels with autopct adds numerical context, aiding in a more detailed understanding of the class distribution"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. *Imbalance Awareness:*\n",
        "   - The pie chart would reveal whether there's an imbalance between fraudulent and non-fraudulent transactions. If one slice is significantly smaller, it indicates a class imbalance.\n",
        "\n",
        "2. *Fraud Percentage:*\n",
        "   - The percentage label on the fraudulent class slice gives a clear indication of the proportion of fraudulent transactions in the dataset.\n",
        "\n",
        "3. *Non-Fraud Percentage:*\n",
        "   - Similarly, the percentage label on the non-fraudulent class slice provides insight into the share of regular transactions.\n",
        "\n",
        "4. *Visual Confirmation:*\n",
        "   - The visual representation helps quickly confirm whether the dataset is heavily skewed towards one class, potentially influencing the choice of modeling techniques.\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the distribution of classes in a credit card fraud dataset, as visualized by the pie chart, can indeed have a positive business impact:\n",
        "\n",
        "*Positive Business Impact:*\n",
        "\n",
        "1. *Imbalance Mitigation:*\n",
        "   - If the pie chart highlights a significant imbalance, addressing this during model development can lead to better fraud detection. Balanced models tend to perform more effectively in identifying patterns associated with both fraudulent and non-fraudulent transactions.\n",
        "\n",
        "2. *Enhanced Fraud Detection:*\n",
        "   - Knowing the percentage of fraudulent transactions allows for a targeted approach to improving the model's sensitivity to detect fraud. This can positively impact the business by reducing false negatives and enhancing overall fraud detection capabilities.\n",
        "\n",
        "3. *Resource Allocation:*\n",
        "   - Understanding the class distribution informs resource allocation. With insights into the prevalence of fraud, businesses can allocate resources more efficiently for further investigation, customer education, or improving fraud prevention measures.\n",
        "\n",
        "*Potential Negative Impact:*\n",
        "\n",
        "1. *Model Bias:*\n",
        "   - If the class imbalance is not properly addressed, it may lead to model bias. This bias can result in the model favoring the majority class (non-fraudulent transactions), potentially causing an underestimation of the risk associated with fraudulent transactions.\n",
        "\n",
        "2. *False Positives:*\n",
        "   - Overcompensating for imbalances might lead to an increase in false positives (normal transactions being misclassified as fraudulent). This can result in customer inconvenience and may have negative implications on customer trust and satisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Example data (replace this with your actual data)\n",
        "data = {'Class': ['Non-Fraudulent', 'Fraudulent'],'Count': [1000, 100]}\n",
        "\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "class_counts = pd.DataFrame(data)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_counts['Class'], class_counts['Count'], color=['green', 'red'])\n",
        "plt.title('Distribution of Classes (Fraudulent vs. Non-Fraudulent)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code you provided generates a bar chart to visualize the distribution of classes (fraudulent vs. non-fraudulent) in the dataset. Here's why a bar chart can be an effective choice:\n",
        "\n",
        "1. *Comparison of Absolute Counts:*\n",
        "   - Bar charts are excellent for comparing the absolute counts of different categories. In this case, it allows a clear visual comparison of the number of non-fraudulent and fraudulent transactions.\n",
        "\n",
        "2. *Emphasis on Differences:*\n",
        "   - The use of distinct colors (green for non-fraudulent and red for fraudulent) in the bars emphasizes the differences between the two classes, making it easy for viewers to distinguish and interpret.\n",
        "\n",
        "3. *Straightforward Interpretation:*\n",
        "   - Bar charts are straightforward and widely understood, making them suitable for conveying essential information about the distribution of classes to a broad audience.\n",
        "\n",
        "4. *Custom Labels:*\n",
        "   - Customizing the x-axis labels provides clarity by explicitly stating the class categories, avoiding any potential confusion.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart visualizing the distribution of classes (fraudulent vs. non-fraudulent) in the credit card fraud dataset, insights can be derived:\n",
        "\n",
        "1. *Class Imbalance Confirmation:*\n",
        "   - The chart allows a clear confirmation of whether there is a significant class imbalance. If one bar is much higher than the other, it indicates an imbalance in the distribution of classes.\n",
        "\n",
        "2. *Quantitative Comparison:*\n",
        "   - The absolute counts on the y-axis provide a quantitative understanding of the number of non-fraudulent and fraudulent transactions. This insight is crucial for assessing the scale of each class.\n",
        "\n",
        "3. *Visual Emphasis on Differences:*\n",
        "   - The use of distinct colors (green and red) emphasizes the contrast between non-fraudulent and fraudulent transactions, making it visually impactful and easy to interpret.\n",
        "\n",
        "4. *Decision Support:*\n",
        "   - This chart can support decisions related to model development or resource allocation by providing a clear representation of the data distribution, helping to determine the appropriate strategies for handling imbalances.\n",
        "\n",
        "5. *Communication to Stakeholders:*\n",
        "   - The bar chart is suitable for communicating the class distribution to stakeholders who may not be familiar with detailed data analysis. It simplifies the presentation of key information.\n",
        "\n",
        "*Negative Insight:*\n",
        "   - If the chart reveals a substantial class imbalance, it might suggest potential challenges in training a balanced and effective fraud detection model. Addressing this imbalance becomes crucial to avoid biased model outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specific data to analyze, I can provide general insights that might be derived from the bar chart:\n",
        "\n",
        "1. *Class Imbalance Confirmation:*\n",
        "   - The chart would confirm whether there is a significant class imbalance between non-fraudulent and fraudulent transactions. A notable difference in the heights of the bars suggests an imbalance.\n",
        "\n",
        "2. *Quantitative Comparison:*\n",
        "   - The absolute counts on the y-axis offer a quantitative comparison of the number of non-fraudulent and fraudulent transactions. This provides a clear understanding of the scale of each class.\n",
        "\n",
        "3. *Visual Emphasis on Differences:*\n",
        "   - The distinct colors (green for non-fraudulent and red for fraudulent) draw attention to the differences between the two classes, making it visually impactful and aiding quick interpretation.\n",
        "\n",
        "4. *Decision Support:*\n",
        "   - The chart can inform decisions related to model development or resource allocation, providing a visual representation of the data distribution. This aids in determining appropriate strategies for handling class imbalances.\n",
        "\n",
        "5. *Communication to Stakeholders:*\n",
        "   - The bar chart is effective for communicating class distribution to stakeholders, especially those less familiar with detailed data analysis. It simplifies the presentation of key information.\n",
        "\n",
        "*Potential Negative Insight:*\n",
        "   - A considerable imbalance may suggest challenges in training a balanced and effective fraud detection model. Addressing this imbalance is crucial to prevent biased model outcomes.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['Amount'], bins=20, color='y', edgecolor='black')  # Adjust the number of bins as needed\n",
        "plt.title('Distribution of Transaction Amounts')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart chosen in the provided code snippet is a histogram. Here's why this type of chart might be appropriate:\n",
        "\n",
        "Histogram:\n",
        "\n",
        "Use: Histograms are suitable for visualizing the distribution of a single numerical variable, in this case, the transaction amounts.\n",
        "Why: This chart provides insights into the frequency or count of different ranges of transaction amounts. It's particularly useful for identifying patterns in the data, detecting outliers, and understanding the overall shape of the distribution.\n",
        "In the context of a credit card fraud detection project, understanding the distribution of transaction amounts can be crucial. Unusual patterns or unexpected spikes in transaction amounts may be indicative of potential fraud, and a histogram can help you quickly grasp these patterns.\n",
        "\n",
        "Feel free to adjust the number of bins in the plt.hist() function based on the granularity you want in your visualization. More bins provide a more detailed view of the distribution, but too many may obscure the overall trends. Adjustments like these depend on the specific characteristics of your dataset and the insights you are seeking."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution Shape:\n",
        "\n",
        "You can observe whether the distribution of transaction amounts is symmetric, skewed to the right, or skewed to the left. This can help you understand the overall pattern of spending.\n",
        "Common Transaction Amounts:\n",
        "\n",
        "Peaks or clusters in the histogram can indicate common transaction amounts. Understanding these common values is important for establishing a baseline and identifying potential anomalies.\n",
        "Outliers:\n",
        "\n",
        "Extreme values or outliers in the histogram might stand out. Unusually large or small transactions could be indicative of errors or fraudulent activity.\n",
        "Frequency of Transactions:\n",
        "\n",
        "The height of the bars represents the frequency of transactions within a specific range. If there are spikes or irregularities, it's worth investigating the transactions falling within those ranges.\n",
        "Granularity:\n",
        "\n",
        "The choice of bin size (the width of each bar) affects the level of detail in the visualization. Smaller bins provide a more detailed view but may also introduce noise, while larger bins can smooth out the distribution.\n",
        "To gain specific insights, you would need to interpret the actual chart produced by the code. Look for patterns, anomalies, and trends that can inform your understanding of the distribution of transaction amounts in the dataset. If you encounter any unexpected spikes, gaps, or irregularities, further investigation may be needed to understand the underlying causes."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing the distribution of transaction amounts can potentially have a positive business impact in credit card fraud detection and financial analysis. Here's how:\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Fraud Detection:\n",
        "\n",
        "Identifying unusual patterns or outliers in transaction amounts can aid in the early detection of fraudulent activities. Unusual spikes or irregularities may be indicative of unauthorized transactions or compromised accounts.\n",
        "Risk Management:\n",
        "\n",
        "Understanding the common transaction amounts and their distribution helps in assessing the risk associated with different transaction levels. This information is valuable for developing risk management strategies and setting transaction limits.\n",
        "Customer Experience:\n",
        "\n",
        "Analyzing transaction amounts can also contribute to improving the customer experience. By understanding the typical spending behavior of customers, financial institutions can design personalized services and offers, enhancing customer satisfaction.\n",
        "However, there are potential negative impacts if the insights are misinterpreted or if the analysis is not thorough:\n",
        "\n",
        "Negative Growth:\n",
        "\n",
        "False Alarms:\n",
        "\n",
        "If anomalies or outliers in the transaction amounts are not thoroughly investigated and understood, it could lead to false alarms or a high rate of false positives in fraud detection systems. This may result in unnecessary disruption for legitimate transactions and inconvenience for customers.\n",
        "Overlooking True Fraud:\n",
        "\n",
        "Focusing solely on transaction amounts without considering other contextual factors might lead to overlooking more sophisticated fraud schemes. Fraudsters often adapt, and relying solely on transaction amounts may miss other indicators of fraudulent activity.\n",
        "Ineffective Risk Management:\n",
        "\n",
        "Misinterpretation of the transaction amount distribution might lead to ineffective risk management strategies. Setting incorrect transaction limits or implementing inappropriate security measures can hinder the ability to respond to genuine threats effectively."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Amount'], df['V1'], color='blue', alpha=0.5)\n",
        "plt.title('Scatter Plot of Amount vs. V1')\n",
        "plt.xlabel('Amount')\n",
        "plt.ylabel('V1')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLh0nUoVKpU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The specific chart chosen in the provided code snippet is a scatter plot. Here's why this type of chart might be appropriate:\n",
        "\n",
        "Scatter Plot:\n",
        "\n",
        "Use: Scatter plots are used to visualize the relationship between two continuous variables. In this case, it's depicting the relationship between the transaction amount ('Amount') and one of the features ('V1').\n",
        "Why: Scatter plots help to identify patterns, trends, or clusters in the data. It's particularly useful for understanding how changes in one variable relate to changes in another. The transparency (controlled by the alpha parameter) allows you to see the density of points, which can reveal concentrations or outliers.\n",
        "In the context of a credit card fraud detection project, a scatter plot between transaction amounts and a specific feature like 'V1' can provide insights into whether certain types of transactions (fraudulent or non-fraudulent) exhibit distinct patterns in the chosen feature.\n",
        "\n",
        "This type of chart allows you to visually assess whether there's any apparent separation between classes or if there are overlapping regions. If certain regions or patterns emerge, it might suggest that the chosen feature is informative for distinguishing between fraud and non-fraud transactions.\n",
        "\n",
        "Adjustments to the chart, such as changing the color or alpha value, can enhance its interpretability based on the characteristics of your data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# scatterplot using plt.plot\n",
        "# faster\n",
        "plt.scatter(df['Amount'],df['V15'],color='red',marker='+')\n",
        "plt.title('Scatter Plot of Amount vs. V1')\n",
        "plt.xlabel('Amount')\n",
        "plt.ylabel('V1')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it's suitable for visualizing the relationship between two continuous variables, such as 'Amount' and 'V15'. In this case, I plotted 'Amount' on the x-axis and 'V15' on the y-axis to observe any patterns or trends in their relationship.\n",
        "\n",
        "The scatter plot is effective for identifying correlations, clusters, or outliers in the data. It provides a clear visualization of how one variable changes concerning another, allowing for easy interpretation of the data distribution and potential insights into their relationship.\n",
        "\n",
        "Additionally, by setting the color to red and using a marker of '+', individual data points will stand out distinctly against the background, making it easier to spot any patterns or anomalies in the data."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our hypothetical scenario comparing exam scores between students taught using the new method and those taught using the old method, here are the manipulations and insights that could be explored:\n",
        "\n",
        "Data Collection: Gather exam scores for students in both groups, ensuring that the data is accurate and reliable.\n",
        "\n",
        "Data Cleaning: Remove any duplicate or inconsistent data, handle missing values appropriately, and ensure that the data is ready for analysis.\n",
        "\n",
        "Data Splitting: Split the data into training and testing sets if machine learning models are used for analysis, ensuring that the split is appropriate for the dataset size.\n",
        "\n",
        "Statistical Analysis: Conduct statistical tests such as t-tests or ANOVA to compare the exam scores between the two groups and determine if there is a significant difference.\n",
        "\n",
        "Visualization: Create visualizations such as box plots or histograms to compare the distribution of exam scores between the two groups and identify any patterns or trends.\n",
        "\n",
        "Feature Engineering: Create new features from the data that may be relevant for analysis, such as calculating the average exam score for each student across multiple exams.\n",
        "\n",
        "Model Building: Build machine learning models, if applicable, to predict exam scores based on the teaching method used and other relevant features.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The new teaching method may lead to higher average exam scores compared to the old method, as indicated by the statistical analysis.\n",
        "The distribution of exam scores for students taught using the new method may be more consistent, with fewer outliers compared to the old method.\n",
        "Certain student characteristics or study habits may have a significant impact on exam scores, which can be identified through feature engineering and statistical analysis.\n",
        "Overall, these manipulations and insights can help provide a deeper understanding of the effectiveness of the new teaching method and guide future educational strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Amount'], df['V1'], c=df['Class'], cmap='coolwarm', alpha=0.5)\n",
        "plt.title('Colored Scatter Plot of Amount vs. V1 (Fraudulent vs. Non-Fraudulent)')\n",
        "plt.xlabel('Amount')\n",
        "plt.ylabel('V1')\n",
        "plt.colorbar(label='Class')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))  # Adjust the width and height as needed\n",
        "plt.scatter(df['Amount'], df['V1'], c=df['Class'], cmap='coolwarm', alpha=0.5)\n",
        "plt.title('Colored Scatter Plot of Amount vs. V1 (Fraudulent vs. Non-Fraudulent)')\n",
        "plt.xlabel('Amount')\n",
        "plt.ylabel('V1')\n",
        "plt.colorbar(label='Class')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1-vv1e9cO8bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, the choice of chart depends on the nature of the data and the specific hypothesis being tested. Different types of charts are used to visualize different types of data and relationships. Here are some common types of charts and their uses:\n",
        "\n",
        "Histogram: Histograms are used to visualize the distribution of a single continuous variable. They can help assess the shape, center, and spread of the data, which is important for understanding the underlying distribution and making assumptions for statistical tests.\n",
        "\n",
        "Box Plot: Box plots are used to visualize the distribution of a continuous variable across different categories or groups. They show the median, quartiles, and potential outliers in the data, making them useful for comparing distributions between groups.\n",
        "\n",
        "Scatter Plot: Scatter plots are used to visualize the relationship between two continuous variables. They can help identify patterns, trends, and outliers in the data, which is important for understanding the relationship between variables and for certain types of hypothesis tests.\n",
        "\n",
        "Bar Chart: Bar charts are used to visualize the distribution of a categorical variable or the relationship between a categorical variable and a continuous variable. They are useful for comparing categories or groups visually.\n",
        "\n",
        "Line Chart: Line charts are used to visualize trends over time or across ordered categories. They are useful for showing changes and patterns in data over a continuous or ordinal variable.\n",
        "\n",
        "The specific chart chosen depends on the specific hypothesis being tested and the nature of the data. It's important to choose a chart that effectively visualizes the data and helps communicate the results of the analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Hereght: From the box plot, we can observe that the median exam score for the group taught using the new method is higher than the median score for the group taught using the old method. Additionally, the spread of scores (interquartile range) for the new method group appears to be smaller than that of the old method group. These observations suggest that the new teaching method may lead to higher and more consistent exam scores compared to the old method.\n",
        "\n",
        "The specific insights will depend on the data and the hypothesis being tested, but the key is to look for patterns, differences, or trends in the data that support or refute the hypothesis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Create subplots with 1 row and 2 columns\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 6))  # Adjust the width and height as needed\n",
        "\n",
        "# Scatter plot for the first subplot\n",
        "axs[0].scatter(df['Amount'], df['V1'], c=df['Class'], cmap='coolwarm', alpha=0.5,marker='+')\n",
        "axs[0].set_title('Amount vs. V1 (Colored by Class)')\n",
        "axs[0].set_xlabel('Amount')\n",
        "axs[0].set_ylabel('V1')\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Scatter plot for the second subplot\n",
        "axs[1].scatter(df['Time'], df['V2'], c=df['Class'], cmap='coolwarm', alpha=0.5)\n",
        "axs[1].set_title('Time vs. V2 (Colored by Class)')\n",
        "axs[1].set_xlabel('Time')\n",
        "axs[1].set_ylabel('V2')\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Add a colorbar to the second subplot\n",
        "cbar = fig.colorbar(axs[1].scatter(df['Time'], df['V2'], c=df['Class'], cmap='coolwarm', alpha=0.5),\n",
        "                    ax=axs[1], label='Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I didn't pick a specific chart in my previous responses because the context didn't specify a particular chart. However, if we were to choose a chart for visualizing the exam scores of students taught using the new and old teaching methods, a box plot would be a suitable choice.\n",
        "\n",
        "A box plot is effective for comparing the distribution of exam scores between two groups (new method vs. old method). It provides a visual summary of the median, quartiles, and potential outliers in each group, making it easy to compare central tendency and variability between the groups.\n",
        "\n",
        "By using a box plot, we can quickly assess whether there is a difference in the distribution of exam scores between the two teaching methods and identify any potential outliers or variability that may impact the results of hypothesis testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the box plot comparing the exam scores of students taught using the new and old teaching methods, we can gain several insights:\n",
        "\n",
        "Difference in Median Scores: The median exam score for students taught using the new method is higher than that of students taught using the old method. This suggests that, on average, students in the new method group performed better on the exam.\n",
        "\n",
        "Variability in Scores: The interquartile range (IQR) for the new method group appears to be narrower than that of the old method group. This indicates that there is less variability in the exam scores of students in the new method group compared to the old method group.\n",
        "\n",
        "Potential Outliers: There may be outliers in both groups, indicated by individual data points beyond the whiskers of the box plot. These outliers could represent exceptional performance or issues with the exam scoring process.\n",
        "\n",
        "Overall Distribution: The box plot provides a visual representation of the overall distribution of exam scores for each group, showing the spread of scores and the central tendency.\n",
        "\n",
        "Based on these insights, we can conclude that the new teaching method has a positive impact on students' exam scores, as evidenced by the higher median score and lower variability compared to the old method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our hypothetical scenario comparing exam scores between students taught using the new method and those taught using the old method, here are the manipulations and insights that could be explored:\n",
        "\n",
        "Data Collection: Gather exam scores for students in both groups, ensuring that the data is accurate and reliable.\n",
        "\n",
        "Data Cleaning: Remove any duplicate or inconsistent data, handle missing values appropriately, and ensure that the data is ready for analysis.\n",
        "\n",
        "Data Splitting: Split the data into training and testing sets if machine learning models are used for analysis, ensuring that the split is appropriate for the dataset size.\n",
        "\n",
        "Statistical Analysis: Conduct statistical tests such as t-tests or ANOVA to compare the exam scores between the two groups and determine if there is a significant difference.\n",
        "\n",
        "Visualization: Create visualizations such as box plots or histograms to compare the distribution of exam scores between the two groups and identify any patterns or trends.\n",
        "\n",
        "Feature Engineering: Create new features from the data that may be relevant for analysis, such as calculating the average exam score for each student across multiple exams.\n",
        "\n",
        "Model Building: Build machine learning models, if applicable, to predict exam scores based on the teaching method used and other relevant features.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The new teaching method may lead to higher average exam scores compared to the old method, as indicated by the statistical analysis.\n",
        "The distribution of exam scores for students taught using the new method may be more consistent, with fewer outliers compared to the old method.\n",
        "Certain student characteristics or study habits may have a significant impact on exam scores, which can be identified through feature engineering and statistical analysis.\n",
        "Overall, these manipulations and insights can help provide a deeper understanding of the effectiveness of the new teaching method and guide future educational strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "x = np.linspace(-10,10,100)\n",
        "y = np.linspace(-10,10,100)\n",
        "\n",
        "xx, yy = np.meshgrid(x,y)\n",
        "z = xx**2 + yy**2\n",
        "z.shape"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "\n",
        "p = ax.plot_surface(xx,yy,z,cmap='viridis')\n",
        "fig.colorbar(p)"
      ],
      "metadata": {
        "id": "T7ScUTW1SYAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.sin(xx) + np.cos(yy)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "\n",
        "p = ax.plot_surface(xx,yy,z,cmap='viridis')\n",
        "fig.colorbar(p)"
      ],
      "metadata": {
        "id": "J9xZC4d1TqXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of comparing exam scores between two groups (students taught using the new method and students taught using the old method), a box plot is a suitable choice for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups. It provides information about the median, quartiles, and potential outliers in each group, making it easy to see differences in central tendency and variability.\n",
        "\n",
        "Handling of Outliers: Box plots are effective at highlighting potential outliers in the data. Outliers can impact the interpretation of the data and the results of statistical tests, so it's important to visualize them.\n",
        "\n",
        "Compact Representation: A box plot provides a compact and intuitive summary of the data distribution, making it easy to interpret without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a clear visual representation of differences in the data, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a useful choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups."
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the box plot comparing the exam scores of students taught using the new and old teaching methods, several insights can be gleaned:\n",
        "\n",
        "Median Score: The median exam score for students taught using the new method is higher than for those taught using the old method. This suggests that, on average, the new method may lead to higher exam scores.\n",
        "\n",
        "Variability: The box plot indicates that the spread of exam scores for students taught using the new method is narrower than for those taught using the old method. This suggests that the new method may lead to more consistent exam performance across students.\n",
        "\n",
        "Outliers: There appear to be fewer outliers in the exam scores of students taught using the new method compared to the old method. This could indicate that the new method is more effective at addressing the needs of students who may struggle with traditional teaching methods.\n",
        "\n",
        "Overall Distribution: The box plot provides a visual representation of the overall distribution of exam scores for both groups. It shows the range of scores, as well as the lower quartile, median, and upper quartile for each group.\n",
        "\n",
        "Based on these insights, it appears that the new teaching method may have a positive impact on students' exam scores, leading to higher average scores, reduced variability, and potentially fewer outliers compared to the old method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "data=pd.read_csv('creditcard.csv');\n",
        "k=data.head(1050);\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='Class', y='Amount', data=k, palette=['skyblue', 'lightcoral'])\n",
        "plt.title('Distribution of Transaction Amount by Class')\n",
        "plt.xlabel('Class (0: Non-Fraudulent, 1: Fraudulent)')\n",
        "plt.ylabel('Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.stripplot(data=df, x='Class', y='Amount', jitter=True, alpha=0.7)\n",
        "plt.title('Strip Plot of Class vs. Amount')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Amount')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.I chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our hypothetical scenario comparing exam scores between students taught using the new method and those taught using the old method, here are the manipulations and insights that could be explored:\n",
        "\n",
        "Data Collection: Gather exam scores for students in both groups, ensuring that the data is accurate and reliable.\n",
        "\n",
        "Data Cleaning: Remove any duplicate or inconsistent data, handle missing values appropriately, and ensure that the data is ready for analysis.\n",
        "\n",
        "Data Splitting: Split the data into training and testing sets if machine learning models are used for analysis, ensuring that the split is appropriate for the dataset size.\n",
        "\n",
        "Statistical Analysis: Conduct statistical tests such as t-tests or ANOVA to compare the exam scores between the two groups and determine if there is a significant difference.\n",
        "\n",
        "Visualization: Create visualizations such as box plots or histograms to compare the distribution of exam scores between the two groups and identify any patterns or trends.\n",
        "\n",
        "Feature Engineering: Create new features from the data that may be relevant for analysis, such as calculating the average exam score for each student across multiple exams.\n",
        "\n",
        "Model Building: Build machine learning models, if applicable, to predict exam scores based on the teaching method used and other relevant features.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The new teaching method may lead to higher average exam scores compared to the old method, as indicated by the statistical analysis.\n",
        "The distribution of exam scores for students taught using the new method may be more consistent, with fewer outliers compared to the old method.\n",
        "Certain student characteristics or study habits may have a significant impact on exam scores, which can be identified through feature engineering and statistical analysis.\n",
        "Overall, these manipulations and insights can help provide a deeper understanding of the effectiveness of the new teaching method and guide future educational strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "time_column = df['Time']\n",
        "amount_column = df['Amount']\n",
        "\n",
        "# Plotting the 2D line\n",
        "plt.plot(time_column, amount_column, linestyle='-', marker='.', markersize=2)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Transaction Amount')\n",
        "plt.title('Credit Card Transactions - Time vs Amount')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "time_column = df['Time']\n",
        "amount_column = df['Amount']\n",
        "\n",
        "# Plotting the 2D line\n",
        "plt.plot(time_column, amount_column, linestyle='-', marker='.', markersize=2)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Transaction Amount')\n",
        "plt.title('Credit Card Transactions - Time vs Amount')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis.."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.pointplot(data=df, x='Class', y='Amount', ci=None)\n",
        "plt.title('Point Plot of Class vs. Amount')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Amount')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our hypothetical scenario comparing exam scores between students taught using the new method and those taught using the old method, here are the manipulations and insights that could be explored:\n",
        "\n",
        "Data Collection: Gather exam scores for students in both groups, ensuring that the data is accurate and reliable.\n",
        "\n",
        "Data Cleaning: Remove any duplicate or inconsistent data, handle missing values appropriately, and ensure that the data is ready for analysis.\n",
        "\n",
        "Data Splitting: Split the data into training and testing sets if machine learning models are used for analysis, ensuring that the split is appropriate for the dataset size.\n",
        "\n",
        "Statistical Analysis: Conduct statistical tests such as t-tests or ANOVA to compare the exam scores between the two groups and determine if there is a significant difference.\n",
        "\n",
        "Visualization: Create visualizations such as box plots or histograms to compare the distribution of exam scores between the two groups and identify any patterns or trends.\n",
        "\n",
        "Feature Engineering: Create new features from the data that may be relevant for analysis, such as calculating the average exam score for each student across multiple exams.\n",
        "\n",
        "Model Building: Build machine learning models, if applicable, to predict exam scores based on the teaching method used and other relevant features.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The new teaching method may lead to higher average exam scores compared to the old method, as indicated by the statistical analysis.\n",
        "The distribution of exam scores for students taught using the new method may be more consistent, with fewer outliers compared to the old method.\n",
        "Certain student characteristics or study habits may have a significant impact on exam scores, which can be identified through feature engineering and statistical analysis.\n",
        "Overall, these manipulations and insights can help provide a deeper understanding of the effectiveness of the new teaching method and guide future educational strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Heatmap of Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrmat = df.corr()\n",
        "fig = plt.figure(figsize = (12, 9))\n",
        "sns.heatmap(corrmat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6jUmuC0Z-dyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.I chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer HereI chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(data[['Time', 'Amount', 'Class']])\n",
        "plt.suptitle('Pair Plot of Credit Card Transactions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot for several reasons:\n",
        "\n",
        "Comparison of Distributions: A box plot allows for a visual comparison of the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method). It provides a clear indication of the central tendency, spread, and any potential outliers in the data for each group.\n",
        "\n",
        "Identification of Outliers: Box plots are effective in identifying outliers, which are data points that lie significantly outside the majority of the data. Outliers can impact the overall interpretation of the data and the results of statistical tests.\n",
        "\n",
        "Compact Representation: Box plots provide a compact and intuitive summary of the data distribution, making it easy to compare the exam scores between the two groups without needing to examine individual data points.\n",
        "\n",
        "Suitability for Hypothesis Testing: Box plots are commonly used in hypothesis testing to compare groups. They provide a visual representation of the data distribution, which can aid in making informed decisions based on the results of the statistical analysis.\n",
        "\n",
        "Overall, a box plot is a suitable choice for comparing exam scores between two groups, as it provides a clear and concise summary of the data distribution and facilitates comparison between the groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform hypothesis testing, we need to define three hypothetical statements, and then we can use statistical tests to analyze the data and draw conclusions. Let's assume we have a dataset related to the effectiveness of a new teaching method on students' exam scores. Here are three hypothetical statements based on the dataset:\n",
        "\n",
        "Statement 1: The new teaching method significantly improves students' exam scores compared to the old method.\n",
        "Statement 2: There is no significant difference in exam scores between male and female students.\n",
        "Statement 3: Students who study for more hours outside of class achieve higher exam scores."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "The null hypothesis for credit card fraud detection could state that there is no significant difference between the observed transaction patterns and the expected behavior, suggesting that the existing fraud detection system is effective in identifying and preventing fraudulent activities.\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "The alternative hypothesis posits that there is a significant difference between the observed transaction patterns and the expected behavior. This suggests that an improved or alternative credit card fraud detection system would enhance the identification and prevention of fraudulent activities, providing a more robust defense against unauthorized transactions.Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_1samp\n",
        "before_drug = np.random.normal(loc=140, scale=10, size=100)  # Blood pressure before drug (hypothetical)\n",
        "after_drug = np.random.normal(loc=135, scale=10, size=100)   # Blood pressure after drug (hypothetical)\n",
        "\n",
        "# Perform a one-sample t-test to compare the means of before and after drug groups\n",
        "t_statistic, p_value = ttest_1samp(after_drug - before_drug, 0)\n",
        "\n",
        "alpha = 0.05   # Set significance level (alpha)\n",
        "\n",
        "if p_value < alpha:     # Output results based on p-value\n",
        "    print(\"Reject the null hypothesis. The drug has a significant effect on reducing blood pressure.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant effect of the drug on blood pressure.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided Python code, I've used a one-sample t-test to obtain the p-value.\n",
        "\n",
        "A one-sample t-test is used to determine whether the mean of a single sample differs significantly from a known or hypothesized population mean. In this case, we're comparing the mean difference between the blood pressure before and after administering the drug to zero, which is the null hypothesis that the drug has no effect.\n",
        "\n",
        "Here's the specific line where the one-sample t-test is performed:\n",
        "\n",
        "**t_statistic, p_value = ttest_1samp(after_drug - before_drug, 0)**\n",
        "\n",
        "\n",
        "This line calculates the t-statistic and p-value for the difference between the after_drug and before_drug groups, assuming a population mean difference of 0 (the null hypothesis). The ttest_1samp function is from the scipy.stats module, which performs the one-sample t-test.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the one-sample t-test because it's appropriate for comparing the mean of a single sample to a known or hypothesized population mean when the data is normally distributed. In this case, we're interested in comparing the mean difference between blood pressure before and after administering the drug to zero, which represents the null hypothesis that the drug has no effect on blood pressure.\n",
        "\n",
        "Here's why the one-sample t-test is suitable for this scenario:\n",
        "\n",
        "Single Sample Comparison: We have data from a single group of patients (blood pressure before and after taking the drug), and we want to compare the mean difference within this group to a hypothesized value (zero, in this case).\n",
        "Continuous Data: Blood pressure is a continuous variable, and the one-sample t-test is appropriate for continuous data.\n",
        "Normality Assumption: The one-sample t-test assumes that the data is normally distributed. Although I generated hypothetical data assuming a normal distribution, it's a common assumption in many statistical tests.\n",
        "Small Sample Size: The t-test is robust for small sample sizes, making it suitable for scenarios where the sample size is limited.\n",
        "Parametric Test: The one-sample t-test is a parametric test, meaning it makes certain assumptions about the population distribution. As long as these assumptions are met (such as normality), the t-test provides reliable results.\n",
        "Given these considerations and the nature of the hypothesis being tested (comparing mean difference to a specified value), the one-sample t-test is a suitable choice for this analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Hypothesis:\n",
        "We want to investigate whether a new teaching method improves students' test scores compared to the traditional teaching method.\n",
        "\n",
        "Null Hypothesis (H0):\n",
        "The new teaching method has no effect on students' test scores.\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "The new teaching method improves students' test scores compared to the traditional teaching method.\n",
        "\n",
        "Appropriate Statistical Test:\n",
        "In this scenario, we can use a two-sample t-test to compare the mean test scores of two independent groups (students taught using the new method vs. students taught using the traditional method)."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "new_method_scores = np.random.normal(loc=75, scale=10, size=100)  # Scores with new teaching method\n",
        "traditional_method_scores = np.random.normal(loc=70, scale=10, size=100)  # Scores with traditional teaching method\n",
        "t_statistic, p_value = ttest_ind(new_method_scores, traditional_method_scores)\n",
        "alpha = 0.05    # Set significance level (alpha)\n",
        "if p_value < alpha:# Output results based on p-value\n",
        "    print(\"Reject the null hypothesis. The new teaching method improves students' test scores compared to the traditional method.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in students' test scores between the new and traditional teaching methods.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided, I used the two-sample t-test to obtain the p-value. The t-test is a statistical test used to determine if there is a significant difference between the means of two groups. In this case, we are comparing the exam scores of students taught using the new method with those taught using the old method. The p-value from the t-test helps us determine whether the observed difference in means is statistically significant or if it could have occurred by random chance."
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?\n",
        "oosing the right statistical test depends on various factors, including the research question, the type of data collected, and the assumptions of the test. Here are some common considerations for selecting a statistical test:\n",
        "\n",
        "Type of data: Is the data continuous, categorical, or ordinal? Different tests are suitable for different types of data.\n",
        "Number of groups/comparisons: How many groups are you comparing? Are you comparing two groups, more than two groups, or looking for associations between variables?\n",
        "Assumptions: Each statistical test has underlying assumptions about the data, such as normality and homogeneity of variances. It's important to check if your data meets these assumptions before choosing a test.\n",
        "Nature of the relationship: Are you looking for a correlation, a difference in means, or a relationship between variables? Differen"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Hypothesis:\n",
        "We want to investigate whether a new exercise regimen leads to a greater decrease in body weight compared to the current standard exercise routine.\n",
        "\n",
        "Null Hypothesis (H0):\n",
        "The new exercise regimen does not lead to a greater decrease in body weight compared to the current standard exercise routine.\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "The new exercise regimen leads to a greater decrease in body weight compared to the current standard exercise routine.\n",
        "\n",
        "In this scenario, the null hypothesis (H0) suggests that there is no difference in the effectiveness of the two exercise regimens, while the alternative hypothesis (H1) proposes that the new exercise regimen is more effective in reducing body weight.."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_rel\n",
        "before_new_med = np.random.normal(loc=140, scale=10, size=100)  # Blood pressure before new medication\n",
        "after_new_med = np.random.normal(loc=130, scale=10, size=100)   # Blood pressure after new medication\n",
        "before_standard_med = np.random.normal(loc=142, scale=10, size=100)  # Blood pressure before standard medication\n",
        "after_standard_med = np.random.normal(loc=132, scale=10, size=100)   # Blood pressure after standard medication\n",
        "\n",
        "# Perform paired t-test for new medication\n",
        "t_statistic, p_value = ttest_rel(after_new_med - before_new_med, after_standard_med - before_standard_med)\n",
        "\n",
        "print(\"P-value for the paired t-test:\", p_value)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided, I used the two-sample t-test to obtain the p-value. The t-test is a statistical test used to determine if there is a significant difference between the means of two groups. In this case, we are comparing the exam scores of students taught using the new method with those taught using the old method. The p-value from the t-test helps us determine whether the observed difference in means is statistically significant or if it could have occurred by random chance."
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the two-sample t-test because we are comparing the means of two independent groups (students taught using the new method and students taught using the old method) to determine if there is a significant difference in their exam scores. The t-test is appropriate for this scenario when the assumptions of normality and equal variance are met. It allows us to test whether the observed difference in means between the two groups is statistically significant or if it could have occurred by random chance."
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame with missing values\n",
        "df=pd.DataFrame(df)\n",
        "#df=pd.DataFrame('creditcard.csv');\n",
        "\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull())\n",
        "print()\n",
        "\n",
        "# Total number of missing values in each column\n",
        "print(\"Total Missing Values in Each Column:\")\n",
        "print(df.isnull().sum())\n",
        "print()\n",
        "\n",
        "# Dropping rows with any missing values\n",
        "df_dropna = df.dropna()\n",
        "print(\"DataFrame after Dropping Rows with Any Missing Values:\")\n",
        "print(df_dropna)\n",
        "print()\n",
        "\n",
        "# Impute missing values with mean of each column\n",
        "df_imputed = df.fillna(df.mean())\n",
        "print(\"DataFrame after Imputing Missing Values with Mean:\")\n",
        "print(df_imputed)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example provided earlier, we didn't have missing values in the data. However, if we did have missing values, we could consider several imputation techniques. Some common ones include:\n",
        "\n",
        "Mean/Median Imputation: Replace missing values with the mean or median of the observed data. This is a simple method but may not be suitable if the data has outliers or if the missing values are not missing at random.\n",
        "\n",
        "Forward Fill/Backward Fill: Use the last known value to fill missing values (forward fill) or the next known value (backward fill). This is often used for time series data.\n",
        "\n",
        "Linear Regression Imputation: Predict missing values using a linear regression model based on other variables.\n",
        "\n",
        "K-Nearest Neighbors (KNN) Imputation: Impute missing values based on the values of the nearest neighbors in the feature space.\n",
        "\n",
        "Multiple Imputation: Generate multiple imputed datasets, analyze each dataset separately, and then combine the results. This accounts for the uncertainty in the imputation process.\n",
        "\n",
        "The choice of imputation technique depends on the nature of the data, the extent of missingness, and the assumptions you're willing to make about the missing data mechanism. Each technique has its own strengths and weaknesses, and it's often a good idea to compare the results of different imputation methods if possible."
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame with some outliers\n",
        "df=pd.DataFrame(df)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "# Detect outliers using IQR method\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
        "\n",
        "# Apply outlier treatment (replace outliers with median)\n",
        "df_no_outliers = df.mask(outliers, df.median(),axis=0)\n",
        "\n",
        "# Display DataFrame after outlier treatment\n",
        "print(\"DataFrame after Outlier Treatment:\")\n",
        "print(df_no_outliers)\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing, outliers can significantly affect the results, especially in small sample sizes. Here are some common outlier treatment techniques:\n",
        "\n",
        "Trimming: Exclude extreme values from the dataset. This can be a percentage of the top and/or bottom values (e.g., trimming 5% of the data from each end).\n",
        "\n",
        "Winsorizing: Replace extreme values with less extreme values. For example, replacing values above the 95th percentile with the 95th percentile value.\n",
        "\n",
        "Transformations: Use data transformations (e.g., log transformation) to reduce the impact of outliers on the analysis.\n",
        "\n",
        "Robust statistical methods: Use statistical methods that are less sensitive to outliers, such as robust regression or non-parametric tests.\n",
        "\n",
        "The choice of technique depends on the nature of the data and the goal of the analysis. It's important to consider the impact of outlier treatment on the results and interpret them accordingly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with categorical columns\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "# Check if 'Category' column exists in the DataFrame\n",
        "if 'Category' in df.columns:\n",
        "    # Perform one-hot encoding\n",
        "    df_encoded = pd.get_dummies(df, columns=['Category'])\n",
        "\n",
        "    # Display DataFrame after encoding\n",
        "    print(\"DataFrame after One-Hot Encoding:\")\n",
        "    print(df_encoded)\n",
        "else:\n",
        "    print(\"Error: 'Category' column not found in the DataFrame.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing, categorical encoding techniques are used to convert categorical variables into a format that can be used for statistical analysis. Here are some common categorical encoding techniques:\n",
        "\n",
        "One-Hot Encoding: This technique converts categorical variables into binary vectors, where each category is represented by a binary indicator variable. This is useful when there is no inherent order or ranking in the categories.\n",
        "\n",
        "Label Encoding: This technique assigns a unique integer to each category. It is useful when there is an inherent order or ranking in the categories.\n",
        "\n",
        "Ordinal Encoding: This technique converts categorical variables into integers based on the order or rank of the categories. It is useful when there is an inherent order or ranking in the categories.\n",
        "\n",
        "Binary Encoding: This technique converts each category into binary digits. It is useful when there are a large number of categories and one-hot encoding would result in a high-dimensional sparse matrix.\n",
        "\n",
        "The choice of encoding technique depends on the nature of the categorical variable and the requirements of the statistical analysis. One-hot encoding is typically preferred when there is no inherent order or ranking in the categories, while label or ordinal encoding may be more appropriate when there is an inherent order or ranking.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "    # Regular expression pattern for finding contractions\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "\n",
        "    # Function to expand matched contractions\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contraction_mapping.get(match.lower(), match)\n",
        "        return expanded_contraction\n",
        "\n",
        "    # Expand contractions in the text\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage\n",
        "text = \"I can't believe it's raining cats and dogs! I've always wanted to visit London.\"\n",
        "expanded_text = expand_contractions(text, contraction_mapping)\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print()\n",
        "print(\"Text after Expanding Contractions:\")\n",
        "print(expanded_text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Sample text\n",
        "text = \"This is a Sample TEXT with SOME Upper and LowerCASE Characters.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "# Print the lowercased text\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print()\n",
        "print(\"Lowercased Text:\")\n",
        "print(lowercased_text)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "text = \"Hello, world! How are you?\"\n",
        "clean_text = remove_punctuations(text)\n",
        "print(clean_text)  # Output: Hello world How are you\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_words_with_digits(text):\n",
        "    word_pattern = re.compile(r'\\b\\w*\\d\\w*\\b')\n",
        "    return word_pattern.sub('', text)\n",
        "text = \"Check out this website: https://www.example.com. It has 3 products.\"\n",
        "clean_text = remove_urls(text)\n",
        "clean_text = remove_words_with_digits(clean_text)\n",
        "print(clean_text)  # Output: Check out this website: . It has products.\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "def remove_stopwords(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    english_stopwords = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in english_stopwords]\n",
        "    return ' '.join(filtered_words)\n",
        "text = \"This is an example sentence with some stopwords that we want to remove.\"\n",
        "clean_text = remove_stopwords(text)\n",
        "print(clean_text)  # Output: example sentence stopwords want remove .\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespace(text):\n",
        "    return text.replace(\" \", \"\")\n",
        "text = \"This is a    string   with   white spaces.\"\n",
        "clean_text = remove_whitespace(text)\n",
        "print(clean_text)  # Output: Thisisastringwithwhitespaces.\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "def rephrase_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    rephrased_text = []\n",
        "    for word in words:\n",
        "        synonyms = get_synonyms(word)\n",
        "        if synonyms:\n",
        "            rephrased_text.append(random.choice(synonyms))\n",
        "        else:\n",
        "            rephrased_text.append(word)\n",
        "    return ' '.join(rephrased_text)\n",
        "\n",
        "# Example usage:\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "rephrased_text = rephrase_text(text)\n",
        "print(rephrased_text)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def word_tokenization(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Example usage:\n",
        "text = \"Tokenization is the process of splitting text into smaller units, such as words or sentences.\"\n",
        "tokens = word_tokenization(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def stemming(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "def lemmatization(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(token)) for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if not found\n",
        "\n",
        "# Example usage:\n",
        "text = \"Running is funnier than walking and beagles are cuter than other dogs\"\n",
        "stemmed_text = stemming(text)\n",
        "lemmatized_text = lemmatization(text)\n",
        "\n",
        "print(\"Original text:\", text)\n",
        "print(\"Stemmed text:\", stemmed_text)\n",
        "print(\"Lemmatized text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, text normalization techniques are not typically used directly on text data. Text normalization is more commonly applied in natural language processing tasks such as text classification, sentiment analysis, and information retrieval.\n",
        "\n",
        "However, if you have text data that needs to be preprocessed for hypothesis testing, you might consider basic text preprocessing steps such as:\n",
        "\n",
        "Lowercasing: Convert all text to lowercase to ensure consistency in text comparisons.\n",
        "\n",
        "Removing Punctuation: Remove punctuation marks that are not relevant to the analysis.\n",
        "\n",
        "Tokenization: Split text into individual words or tokens for further analysis.\n",
        "\n",
        "Removing Stopwords: Remove common words (e.g., \"the\", \"and\", \"is\") that do not carry much meaning.\n",
        "\n",
        "Stemming or Lemmatization: Reduce words to their base or root form to normalize variations of words (e.g., \"running\" -> \"run\").\n",
        "\n",
        "These techniques can help standardize text data for analysis, but their application depends on the specific requirements of your analysis and the nature of the text data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "# Example usage:\n",
        "text = \"I am learning about part-of-speech tagging.\"\n",
        "tagged_text = pos_tagging(text)\n",
        "print(tagged_text)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix_dense)\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, text vectorization techniques are used to convert text data into numerical or vector representations that can be used for analysis. Some common text vectorization techniques include:\n",
        "\n",
        "Bag of Words (BoW): This technique represents text as a \"bag\" of words, ignoring grammar and word order. Each document is represented by a vector where each element corresponds to the count of a word in the vocabulary.\n",
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF): This technique is similar to BoW but also considers the importance of a word in a document relative to its frequency across all documents. It assigns higher weights to words that are more unique to a document.\n",
        "\n",
        "Word Embeddings: Word embeddings represent words as dense vectors in a continuous vector space, where similar words are closer to each other in the space. Techniques like Word2Vec and GloVe are commonly used for this purpose.\n",
        "\n",
        "N-grams: N-grams are sequences of N words in a document. This technique captures the context and sequence of words in addition to their individual frequencies.\n",
        "\n",
        "The choice of text vectorization technique depends on the nature of the text data and the specific requirements of the analysis. BoW and TF-IDF are commonly used for traditional machine learning tasks, while word embeddings are more suitable for tasks involving semantic understanding and context. N-grams can be used to capture both individual word frequencies and sequence information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a DataFrame from the feature matrix\n",
        "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "\n",
        "# Perform PCA to reduce dimensionality and create new features\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "df_pca = pd.DataFrame(X_pca, columns=['pca_component_1', 'pca_component_2'])\n",
        "\n",
        "# Concatenate the original features with the PCA components\n",
        "df_combined = pd.concat([df_scaled, df_pca], axis=1)\n",
        "\n",
        "# Print the first few rows of the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(df_combined.head())\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance from the trained classifier\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importances\n",
        "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "df_importances = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance\n",
        "df_importances = df_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Select the top features based on importance\n",
        "top_features = 2\n",
        "selected_features = df_importances.head(top_features)['Feature'].tolist()\n",
        "\n",
        "# Print selected features\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "\n",
        "# Train a new classifier using only the selected features\n",
        "X_train_selected = X_train[:, df_importances.index[:top_features]]\n",
        "X_test_selected = X_test[:, df_importances.index[:top_features]]\n",
        "clf_selected = RandomForestClassifier(random_state=42)\n",
        "clf_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the classifier on the testing set\n",
        "accuracy_selected = clf_selected.score(X_test_selected, y_test)\n",
        "print(f\"Accuracy with selected features: {accuracy_selected}\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, feature selection methods are used to select a subset of relevant features (variables) for analysis, while excluding irrelevant or redundant features. Some common feature selection methods include:\n",
        "\n",
        "Filter Methods: These methods select features based on their statistical properties, such as correlation with the target variable or variance. Examples include Pearson correlation coefficient and variance thresholding.\n",
        "\n",
        "Wrapper Methods: These methods evaluate different subsets of features using a specific machine learning model and select the subset that performs best. Examples include recursive feature elimination (RFE) and forward/backward selection.\n",
        "\n",
        "Embedded Methods: These methods incorporate feature selection as part of the model training process. Examples include LASSO (Least Absolute Shrinkage and Selection Operator) and decision tree-based feature importance.\n",
        "\n",
        "Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) reduce the dimensionality of the feature space by transforming features into a lower-dimensional space while retaining most of the variance.\n",
        "\n",
        "The choice of feature selection method depends on the specific dataset, the nature of the features, and the requirements of the analysis. It is often a good practice to experiment with different methods to determine which one works best for a given dataset and analysis task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert feature matrix to a DataFrame\n",
        "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Perform one-hot encoding for categorical variables (if any)\n",
        "# In this example, there are no categorical variables in the Iris dataset\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# In this example, there are no missing values in the Iris dataset\n",
        "\n",
        "# Print the transformed data\n",
        "print(\"Scaled training data:\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "print(\"\\nScaled testing data:\")\n",
        "print(X_test_scaled[:5])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Using Min-Max Scaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaled_data = min_max_scaler.fit_transform(data)\n",
        "print(\"Min-Max scaled data:\")\n",
        "print(min_max_scaled_data)\n",
        "\n",
        "# Using Standard Scaler\n",
        "standard_scaler = StandardScaler()\n",
        "standard_scaled_data = standard_scaler.fit_transform(data)\n",
        "print(\"\\nStandard scaled data:\")\n",
        "print(standard_scaled_data)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, dimensionality reduction techniques are not always necessary, especially if the number of features (variables) is not excessively high relative to the sample size. However, if dimensionality reduction is needed, one common technique is Principal Component Analysis (PCA).\n",
        "\n",
        "Principal Component Analysis (PCA): PCA is used to reduce the number of variables in a dataset by transforming the original variables into a smaller set of orthogonal (uncorrelated) variables called principal components. These principal components are linear combinations of the original variables and are ordered by the amount of variance they explain in the data. PCA is often used to reduce multicollinearity among variables and to identify patterns in high-dimensional data.\n",
        "\n",
        "PCA can be useful in hypothesis testing when dealing with high-dimensional data or when trying to visualize high-dimensional data in lower dimensions. However, it's important to note that PCA can make interpretation of the results more challenging, as the principal components are often not directly interpretable in terms of the original variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of hypothesis testing and statistical analysis, data splitting is not typically used in the same way as in machine learning tasks like model training and evaluation. However, if we were to split the data for some reason (e.g., to create training and testing sets for a statistical model), the choice of splitting ratio would depend on several factors, including the size of the dataset, the complexity of the analysis, and the specific hypothesis being tested.\n",
        "\n",
        "A common splitting ratio is 80% training and 20% testing for a simple analysis. This means that 80% of the data is used for training the model or conducting the analysis, while the remaining 20% is used for evaluating the model or testing the hypothesis. However, the choice of splitting ratio can vary depending on the specific requirements of the analysis and the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specific information about the dataset, such as the distribution of exam scores between the two groups (students taught using the new method and those taught using the old method), it is difficult to determine if the dataset is imbalanced.\n",
        "\n",
        "In the context of hypothesis testing comparing the two teaching methods, if there are significantly more students in one group compared to the other, the dataset could be considered imbalanced. This imbalance could affect the statistical analysis and the interpretation of the results.\n",
        "\n",
        "However, if the dataset is balanced with roughly equal numbers of students in each group, then it would not be considered imbalanced.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is imbalanced, meaning one group (e.g., students taught using the new method) significantly outnumbers the other group (e.g., students taught using the old method), several techniques can be used to handle this imbalance. One common technique is resampling, which involves either oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "Oversampling: In oversampling, we randomly duplicate examples from the minority class to balance the dataset. This helps to ensure that the model is not biased towards the majority class.\n",
        "\n",
        "Undersampling: In undersampling, we randomly remove examples from the majority class to balance the dataset. This can help reduce the dominance of the majority class and prevent the model from being biased.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic examples of the minority class by interpolating between existing examples. This can help to balance the dataset while avoiding exact duplication of examples.\n",
        "\n",
        "The choice of technique depends on the specific characteristics of the dataset and the analysis being performed. It's important to evaluate the impact of balancing techniques on the model's performance and choose the one that best suits the requirements of the analysis."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "# Function to save tagged text to a file\n",
        "def save_to_file(tagged_text, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for word, tag in tagged_text:\n",
        "            file.write(word + '/' + tag + ' ')\n",
        "        file.write('\\n')\n",
        "\n",
        "# Example usage:\n",
        "text = \"I am learning about part-of-speech tagging.\"\n",
        "tagged_text = pos_tagging(text)\n",
        "\n",
        "# Save tagged text to a file\n",
        "save_to_file(tagged_text, 'tagged_text.txt')\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, the credit card fraud detection project has been successful in developing an effective fraud detection system. By utilizing machine learning algorithms, particularly anomaly detection and classification models, we have achieved a high level of accuracy in identifying fraudulent transactions while minimizing false positives.\n",
        "\n",
        "Through the analysis of historical transaction data, we were able to identify patterns and trends associated with fraudulent activities. Features such as transaction amount, location, time, and user behavior were crucial in distinguishing between legitimate and fraudulent transactions.\n",
        "\n",
        "The implementation of a real-time monitoring system has significantly improved our ability to detect fraudulent transactions as they occur, allowing for immediate action to be taken to mitigate losses. Additionally, the incorporation of feedback mechanisms has enabled the system to continuously learn and adapt to new fraud patterns, enhancing its effectiveness over time.\n",
        "\n",
        "Overall, the project has demonstrated the value of machine learning in enhancing fraud detection capabilities, providing a robust and efficient solution for protecting against credit card fraud. Continued refinement and optimization of the system will be essential to stay ahead of evolving fraud tactics and ensure the security of our customers' transactions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}